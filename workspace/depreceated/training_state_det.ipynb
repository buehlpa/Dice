{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 289 images belonging to 3 classes.\n",
      "Found 71 images belonging to 3 classes.\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 3s 240ms/step - loss: 1.2384 - accuracy: 0.6809 - val_loss: 0.6308 - val_accuracy: 0.8281\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 2s 189ms/step - loss: 0.2903 - accuracy: 0.8755 - val_loss: 0.2313 - val_accuracy: 0.8750\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 2s 191ms/step - loss: 0.1854 - accuracy: 0.9105 - val_loss: 0.2778 - val_accuracy: 0.9062\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 2s 189ms/step - loss: 0.1194 - accuracy: 0.9416 - val_loss: 0.3001 - val_accuracy: 0.9062\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 2s 190ms/step - loss: 0.1134 - accuracy: 0.9494 - val_loss: 0.2865 - val_accuracy: 0.9062\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 2s 188ms/step - loss: 0.1243 - accuracy: 0.9455 - val_loss: 0.3103 - val_accuracy: 0.9062\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 2s 188ms/step - loss: 0.1227 - accuracy: 0.9533 - val_loss: 0.3261 - val_accuracy: 0.9062\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 2s 190ms/step - loss: 0.1137 - accuracy: 0.9572 - val_loss: 0.3332 - val_accuracy: 0.9219\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 2s 189ms/step - loss: 0.0776 - accuracy: 0.9728 - val_loss: 0.2767 - val_accuracy: 0.9219\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 2s 188ms/step - loss: 0.0841 - accuracy: 0.9728 - val_loss: 0.4016 - val_accuracy: 0.8906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f6a441966d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train for sequence of images\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "\n",
    "# Set parameters\n",
    "img_width, img_height = 224, 224\n",
    "batch_size = 32\n",
    "\n",
    "# Load the MobileNetV2 model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))  # Using 3 channels here\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Define the custom head for our network\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')  # Assuming 3 classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Data generators for training and validation\n",
    "# Note: Using `preprocess_input` from MobileNetV2 for preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,  # Preprocess suitable for MobileNetV2\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/home/bule/projects/Dice/data/eyes2',  # Change to your data directory\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb',  # Changed to 'rgb'\n",
    "    subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    '/home/bule/projects/Dice/data/eyes2',  # Change to your data directory\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb',  # Changed to 'rgb'\n",
    "    subset='validation')\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    epochs=10  # Adjust the number of epochs\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/bule/projects/Dice/data/eyes2/states'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/bule/projects/Dice/training_state_det.ipynb Cell 2\u001b[0m line \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/Dice/training_state_det.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m# Create an instance of the ImageDataGenerator class\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/Dice/training_state_det.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m train_datagen \u001b[39m=\u001b[39m ImageDataGenerator(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/Dice/training_state_det.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m     shear_range\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m,  \u001b[39m# Shear angle in counter-clockwise direction as radians\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/Dice/training_state_det.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m     preprocessing_function\u001b[39m=\u001b[39mcustom_preprocess_input,  \u001b[39m# Custom preprocess function\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/Dice/training_state_det.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m     validation_split\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/Dice/training_state_det.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/Dice/training_state_det.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m train_generator \u001b[39m=\u001b[39m train_datagen\u001b[39m.\u001b[39;49mflow_from_directory(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/Dice/training_state_det.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39m/home/bule/projects/Dice/data/eyes2/states\u001b[39;49m\u001b[39m'\u001b[39;49m,  \u001b[39m# Change to your data directory\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/Dice/training_state_det.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m     target_size\u001b[39m=\u001b[39;49m(img_width, img_height),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/Dice/training_state_det.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/Dice/training_state_det.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m     class_mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcategorical\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/Dice/training_state_det.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m     color_mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrgb\u001b[39;49m\u001b[39m'\u001b[39;49m,  \u001b[39m# Changed to 'rgb'\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/Dice/training_state_det.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m     subset\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtraining\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/Dice/training_state_det.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m validation_generator \u001b[39m=\u001b[39m train_datagen\u001b[39m.\u001b[39mflow_from_directory(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/Dice/training_state_det.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m/home/bule/projects/Dice/data/eyes2/states\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m# Change to your data directory\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/Dice/training_state_det.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m     target_size\u001b[39m=\u001b[39m(img_width, img_height),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/Dice/training_state_det.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m     color_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrgb\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m# Changed to 'rgb'\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/Dice/training_state_det.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m     subset\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/Dice/training_state_det.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dice/lib/python3.9/site-packages/keras/src/preprocessing/image.py:1649\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1562\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflow_from_directory\u001b[39m(\n\u001b[1;32m   1563\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1564\u001b[0m     directory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1578\u001b[0m     keep_aspect_ratio\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1579\u001b[0m ):\n\u001b[1;32m   1580\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Takes the path to a directory & generates batches of augmented data.\u001b[39;00m\n\u001b[1;32m   1581\u001b[0m \n\u001b[1;32m   1582\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[39m        and `y` is a numpy array of corresponding labels.\u001b[39;00m\n\u001b[1;32m   1648\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1649\u001b[0m     \u001b[39mreturn\u001b[39;00m DirectoryIterator(\n\u001b[1;32m   1650\u001b[0m         directory,\n\u001b[1;32m   1651\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1652\u001b[0m         target_size\u001b[39m=\u001b[39;49mtarget_size,\n\u001b[1;32m   1653\u001b[0m         color_mode\u001b[39m=\u001b[39;49mcolor_mode,\n\u001b[1;32m   1654\u001b[0m         keep_aspect_ratio\u001b[39m=\u001b[39;49mkeep_aspect_ratio,\n\u001b[1;32m   1655\u001b[0m         classes\u001b[39m=\u001b[39;49mclasses,\n\u001b[1;32m   1656\u001b[0m         class_mode\u001b[39m=\u001b[39;49mclass_mode,\n\u001b[1;32m   1657\u001b[0m         data_format\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_format,\n\u001b[1;32m   1658\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1659\u001b[0m         shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   1660\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m   1661\u001b[0m         save_to_dir\u001b[39m=\u001b[39;49msave_to_dir,\n\u001b[1;32m   1662\u001b[0m         save_prefix\u001b[39m=\u001b[39;49msave_prefix,\n\u001b[1;32m   1663\u001b[0m         save_format\u001b[39m=\u001b[39;49msave_format,\n\u001b[1;32m   1664\u001b[0m         follow_links\u001b[39m=\u001b[39;49mfollow_links,\n\u001b[1;32m   1665\u001b[0m         subset\u001b[39m=\u001b[39;49msubset,\n\u001b[1;32m   1666\u001b[0m         interpolation\u001b[39m=\u001b[39;49minterpolation,\n\u001b[1;32m   1667\u001b[0m         dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype,\n\u001b[1;32m   1668\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/dice/lib/python3.9/site-packages/keras/src/preprocessing/image.py:563\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[1;32m    562\u001b[0m     classes \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 563\u001b[0m     \u001b[39mfor\u001b[39;00m subdir \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(os\u001b[39m.\u001b[39;49mlistdir(directory)):\n\u001b[1;32m    564\u001b[0m         \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    565\u001b[0m             classes\u001b[39m.\u001b[39mappend(subdir)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/bule/projects/Dice/data/eyes2/states'"
     ]
    }
   ],
   "source": [
    "# Train for sequence of images\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "\n",
    "# Set parameters\n",
    "img_width, img_height = 224, 224\n",
    "batch_size = 32\n",
    "\n",
    "# Load the MobileNetV2 model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))  # Using 3 channels here\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Define the custom head for our network\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')  # Assuming 3 classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Function to add noise\n",
    "def add_noise(img):\n",
    "    '''Add random noise to an image'''\n",
    "    row, col, ch = img.shape\n",
    "    s_vs_p = 0.5\n",
    "    amount = 0.04\n",
    "    out = np.copy(img)\n",
    "    # Salt mode\n",
    "    num_salt = np.ceil(amount * img.size * s_vs_p)\n",
    "    coords = [np.random.randint(0, i - 1, int(num_salt))\n",
    "              for i in img.shape]\n",
    "    out[coords] = 1\n",
    "\n",
    "    # Pepper mode\n",
    "    num_pepper = np.ceil(amount* img.size * (1. - s_vs_p))\n",
    "    coords = [np.random.randint(0, i - 1, int(num_pepper))\n",
    "              for i in img.shape]\n",
    "    out[coords] = 0\n",
    "    return out\n",
    "\n",
    "# Function to add blur\n",
    "def add_blur(img):\n",
    "    '''Add random blur to an image'''\n",
    "    return cv2.GaussianBlur(img, (5, 5), 0)\n",
    "\n",
    "# Custom preprocessing function\n",
    "def custom_preprocess_input(img):\n",
    "    img = preprocess_input(img)  # preprocess suitable for MobileNetV2\n",
    "    # img = add_noise(img)  # Add random noise\n",
    "    #img = add_blur(img)  # Add random blur\n",
    "    return img\n",
    "# Create an instance of the ImageDataGenerator class\n",
    "train_datagen = ImageDataGenerator(\n",
    "    shear_range=15,  # Shear angle in counter-clockwise direction as radians\n",
    "    preprocessing_function=custom_preprocess_input,  # Custom preprocess function\n",
    "    validation_split=0.2\n",
    ")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/home/bule/projects/Dice/data/eyes2/states',  # Change to your data directory\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb',  # Changed to 'rgb'\n",
    "    subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    '/home/bule/projects/Dice/data/eyes2/states',  # Change to your data directory\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb',  # Changed to 'rgb'\n",
    "    subset='validation')\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    epochs=10  # Adjust the number of epochs\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n",
      "WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp0rqnxsx0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp0rqnxsx0/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to TensorFlow Lite and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-04 20:25:25.372475: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-11-04 20:25:25.372492: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-11-04 20:25:25.372620: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp0rqnxsx0\n",
      "2023-11-04 20:25:25.385446: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-11-04 20:25:25.385458: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp0rqnxsx0\n",
      "2023-11-04 20:25:25.419133: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-11-04 20:25:25.627811: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp0rqnxsx0\n",
      "2023-11-04 20:25:25.706321: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 333700 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TensorFlow Lite model to a file\n",
    "with open('model_dices_eyes2.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model converted to TensorFlow Lite and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for dices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 141 images belonging to 6 classes.\n",
      "Found 32 images belonging to 6 classes.\n",
      "Epoch 1/40\n",
      "4/4 [==============================] - 2s 291ms/step - loss: 2.8328 - accuracy: 0.2844 - val_loss: 2.1592 - val_accuracy: 0.3438\n",
      "Epoch 2/40\n",
      "4/4 [==============================] - 1s 162ms/step - loss: 2.0232 - accuracy: 0.3486 - val_loss: 1.8857 - val_accuracy: 0.2500\n",
      "Epoch 3/40\n",
      "4/4 [==============================] - 1s 211ms/step - loss: 1.4730 - accuracy: 0.5138 - val_loss: 2.0892 - val_accuracy: 0.2812\n",
      "Epoch 4/40\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 1.5773 - accuracy: 0.5046 - val_loss: 1.8166 - val_accuracy: 0.3750\n",
      "Epoch 5/40\n",
      "4/4 [==============================] - 1s 191ms/step - loss: 1.3417 - accuracy: 0.5312 - val_loss: 1.9780 - val_accuracy: 0.2812\n",
      "Epoch 6/40\n",
      "4/4 [==============================] - 1s 169ms/step - loss: 1.2462 - accuracy: 0.5046 - val_loss: 1.9321 - val_accuracy: 0.2500\n",
      "Epoch 7/40\n",
      "4/4 [==============================] - 1s 186ms/step - loss: 1.0309 - accuracy: 0.5781 - val_loss: 1.9352 - val_accuracy: 0.2812\n",
      "Epoch 8/40\n",
      "4/4 [==============================] - 1s 159ms/step - loss: 0.9751 - accuracy: 0.6697 - val_loss: 1.9605 - val_accuracy: 0.2812\n",
      "Epoch 9/40\n",
      "4/4 [==============================] - 1s 167ms/step - loss: 1.0111 - accuracy: 0.6330 - val_loss: 1.9959 - val_accuracy: 0.4062\n",
      "Epoch 10/40\n",
      "4/4 [==============================] - 1s 165ms/step - loss: 0.9396 - accuracy: 0.6514 - val_loss: 1.9306 - val_accuracy: 0.3750\n",
      "Epoch 11/40\n",
      "4/4 [==============================] - 1s 168ms/step - loss: 0.7637 - accuracy: 0.7431 - val_loss: 2.0920 - val_accuracy: 0.3438\n",
      "Epoch 12/40\n",
      "4/4 [==============================] - 1s 167ms/step - loss: 0.9269 - accuracy: 0.6789 - val_loss: 2.0927 - val_accuracy: 0.2812\n",
      "Epoch 13/40\n",
      "4/4 [==============================] - 1s 169ms/step - loss: 0.8011 - accuracy: 0.6972 - val_loss: 2.1598 - val_accuracy: 0.3125\n",
      "Epoch 14/40\n",
      "4/4 [==============================] - 1s 204ms/step - loss: 0.8272 - accuracy: 0.7156 - val_loss: 2.1689 - val_accuracy: 0.2812\n",
      "Epoch 15/40\n",
      "4/4 [==============================] - 1s 165ms/step - loss: 0.7774 - accuracy: 0.7706 - val_loss: 1.8784 - val_accuracy: 0.3438\n",
      "Epoch 16/40\n",
      "4/4 [==============================] - 1s 165ms/step - loss: 0.7143 - accuracy: 0.7248 - val_loss: 1.8680 - val_accuracy: 0.4062\n",
      "Epoch 17/40\n",
      "4/4 [==============================] - 1s 167ms/step - loss: 0.6705 - accuracy: 0.7798 - val_loss: 2.1731 - val_accuracy: 0.2812\n",
      "Epoch 18/40\n",
      "4/4 [==============================] - 1s 165ms/step - loss: 0.6696 - accuracy: 0.7706 - val_loss: 2.0548 - val_accuracy: 0.3125\n",
      "Epoch 19/40\n",
      "4/4 [==============================] - 1s 202ms/step - loss: 0.5526 - accuracy: 0.8532 - val_loss: 1.9265 - val_accuracy: 0.4062\n",
      "Epoch 20/40\n",
      "4/4 [==============================] - 1s 166ms/step - loss: 0.6000 - accuracy: 0.8073 - val_loss: 2.1812 - val_accuracy: 0.2812\n",
      "Epoch 21/40\n",
      "4/4 [==============================] - 1s 204ms/step - loss: 0.6310 - accuracy: 0.7706 - val_loss: 2.0828 - val_accuracy: 0.3750\n",
      "Epoch 22/40\n",
      "4/4 [==============================] - 1s 205ms/step - loss: 0.5781 - accuracy: 0.8073 - val_loss: 1.9949 - val_accuracy: 0.3438\n",
      "Epoch 23/40\n",
      "4/4 [==============================] - 1s 188ms/step - loss: 0.4695 - accuracy: 0.8750 - val_loss: 2.0292 - val_accuracy: 0.4062\n",
      "Epoch 24/40\n",
      "4/4 [==============================] - 1s 199ms/step - loss: 0.4986 - accuracy: 0.8359 - val_loss: 2.1225 - val_accuracy: 0.3750\n",
      "Epoch 25/40\n",
      "4/4 [==============================] - 1s 158ms/step - loss: 0.4490 - accuracy: 0.8716 - val_loss: 2.1025 - val_accuracy: 0.3750\n",
      "Epoch 26/40\n",
      "4/4 [==============================] - 1s 196ms/step - loss: 0.3740 - accuracy: 0.9062 - val_loss: 2.0358 - val_accuracy: 0.4062\n",
      "Epoch 27/40\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 0.3202 - accuracy: 0.9450 - val_loss: 2.1061 - val_accuracy: 0.4375\n",
      "Epoch 28/40\n",
      "4/4 [==============================] - 1s 163ms/step - loss: 0.3414 - accuracy: 0.9358 - val_loss: 2.2766 - val_accuracy: 0.3750\n",
      "Epoch 29/40\n",
      "4/4 [==============================] - 1s 187ms/step - loss: 0.3740 - accuracy: 0.9062 - val_loss: 2.2289 - val_accuracy: 0.3750\n",
      "Epoch 30/40\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 0.3708 - accuracy: 0.8807 - val_loss: 2.1108 - val_accuracy: 0.4062\n",
      "Epoch 31/40\n",
      "4/4 [==============================] - 1s 158ms/step - loss: 0.3594 - accuracy: 0.8899 - val_loss: 2.1570 - val_accuracy: 0.4062\n",
      "Epoch 32/40\n",
      "4/4 [==============================] - 1s 196ms/step - loss: 0.3200 - accuracy: 0.9141 - val_loss: 2.2675 - val_accuracy: 0.4062\n",
      "Epoch 33/40\n",
      "4/4 [==============================] - 1s 164ms/step - loss: 0.3181 - accuracy: 0.9358 - val_loss: 2.1011 - val_accuracy: 0.4375\n",
      "Epoch 34/40\n",
      "4/4 [==============================] - 1s 166ms/step - loss: 0.2121 - accuracy: 0.9725 - val_loss: 2.2081 - val_accuracy: 0.4375\n",
      "Epoch 35/40\n",
      "4/4 [==============================] - 1s 165ms/step - loss: 0.3034 - accuracy: 0.8899 - val_loss: 2.1572 - val_accuracy: 0.4062\n",
      "Epoch 36/40\n",
      "4/4 [==============================] - 1s 201ms/step - loss: 0.2706 - accuracy: 0.9266 - val_loss: 2.3313 - val_accuracy: 0.3750\n",
      "Epoch 37/40\n",
      "4/4 [==============================] - 1s 163ms/step - loss: 0.2371 - accuracy: 0.9450 - val_loss: 2.1853 - val_accuracy: 0.3750\n",
      "Epoch 38/40\n",
      "4/4 [==============================] - 1s 155ms/step - loss: 0.2520 - accuracy: 0.9450 - val_loss: 2.2689 - val_accuracy: 0.4375\n",
      "Epoch 39/40\n",
      "4/4 [==============================] - 1s 193ms/step - loss: 0.2565 - accuracy: 0.9297 - val_loss: 2.3343 - val_accuracy: 0.4062\n",
      "Epoch 40/40\n",
      "4/4 [==============================] - 1s 164ms/step - loss: 0.2432 - accuracy: 0.9725 - val_loss: 2.2075 - val_accuracy: 0.4375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f31715a04f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train for sequence of images\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "\n",
    "# Set parameters\n",
    "img_width, img_height = 224, 224\n",
    "batch_size = 32\n",
    "\n",
    "# Load the MobileNetV2 model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))  # Using 3 channels here\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Define the custom head for our network\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(6, activation='softmax')  # Assuming 3 classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Data generators for training and validation\n",
    "# Note: Using `preprocess_input` from MobileNetV2 for preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,  # Preprocess suitable for MobileNetV2\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/home/bule/projects/Dice/data_still_numbers',  # Change to your data directory\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb',  # Changed to 'rgb'\n",
    "    subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    '/home/bule/projects/Dice/data_still_numbers',  # Change to your data directory\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb',  # Changed to 'rgb'\n",
    "    subset='validation')\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    epochs=40  # Adjust the number of epochs\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmphl8bjbo4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmphl8bjbo4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to TensorFlow Lite and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-02 17:13:09.152198: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-11-02 17:13:09.152215: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-11-02 17:13:09.152344: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmphl8bjbo4\n",
      "2023-11-02 17:13:09.162252: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-11-02 17:13:09.162264: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmphl8bjbo4\n",
      "2023-11-02 17:13:09.193014: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-11-02 17:13:09.399998: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmphl8bjbo4\n",
      "2023-11-02 17:13:09.477801: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 325457 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TensorFlow Lite model to a file\n",
    "with open('model.tflite_classifierdices', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model converted to TensorFlow Lite and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run on raspy with tflite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tflite_runtime.interpreter as tflite\n",
    "\n",
    "# Define class labels for the first and second models\n",
    "class_labels_state = [\"empty\", \"rolling\", \"still\"]\n",
    "class_labels_dice = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
    "\n",
    "# Load the TFLite models\n",
    "interpreter_state = tflite.Interpreter(model_path=\"model2.tflite\")\n",
    "interpreter_state.allocate_tensors()\n",
    "\n",
    "interpreter_dice = tflite.Interpreter(model_path=\"model_classifierdices.tflite\")\n",
    "interpreter_dice.allocate_tensors()\n",
    "\n",
    "# Get model details\n",
    "input_details_state = interpreter_state.get_input_details()\n",
    "output_details_state = interpreter_state.get_output_details()\n",
    "input_shape_state = input_details_state[0]['shape']\n",
    "\n",
    "input_details_dice = interpreter_dice.get_input_details()\n",
    "output_details_dice = interpreter_dice.get_output_details()\n",
    "input_shape_dice = input_details_dice[0]['shape']\n",
    "\n",
    "# Open a handle to the default webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB, then resize and preprocess it\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_resized = cv2.resize(frame_rgb, (224, 224))  # Assuming both models use the same input size\n",
    "    input_data = np.expand_dims(frame_resized, axis=0)\n",
    "    if input_details_state[0]['dtype'] == np.float32: \n",
    "        input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "\n",
    "    # Feed the frame to the state model\n",
    "    interpreter_state.set_tensor(input_details_state[0]['index'], input_data)\n",
    "    interpreter_state.invoke()\n",
    "\n",
    "    # Retrieve the results and determine the class with the highest probability\n",
    "    output_data_state = interpreter_state.get_tensor(output_details_state[0]['index'])\n",
    "    prediction_state = np.argmax(output_data_state)\n",
    "    class_label_state = class_labels_state[prediction_state]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Run the dice model only if the state is \"still\"\n",
    "    dice_label = \"\"\n",
    "    if class_label_state == \"still\":\n",
    "        interpreter_dice.set_tensor(input_details_dice[0]['index'], input_data)\n",
    "        interpreter_dice.invoke()\n",
    "        output_data_dice = interpreter_dice.get_tensor(output_details_dice[0]['index'])\n",
    "        prediction_dice = np.argmax(output_data_dice)\n",
    "        dice_label = class_labels_dice[prediction_dice]\n",
    "\n",
    "    # Display the resulting frame with the class label\n",
    "    cv2.putText(frame, f'State: {class_label_state}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    if dice_label:\n",
    "        cv2.putText(frame, f'Dice: {dice_label}', (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
