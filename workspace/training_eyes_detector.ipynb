{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train a model to predict what the single dice classes are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 230 images belonging to 6 classes.\n",
      "Found 55 images belonging to 6 classes.\n",
      "Epoch 1/40\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.7713 - accuracy: 0.2677 - val_loss: 1.6981 - val_accuracy: 0.3438\n",
      "Epoch 2/40\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.7427 - accuracy: 0.2677 - val_loss: 1.7654 - val_accuracy: 0.2812\n",
      "Epoch 3/40\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 1.7306 - accuracy: 0.2475 - val_loss: 1.7058 - val_accuracy: 0.2500\n",
      "Epoch 4/40\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 1.6981 - accuracy: 0.2475 - val_loss: 1.7306 - val_accuracy: 0.2188\n",
      "Epoch 5/40\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1.6719 - accuracy: 0.2778 - val_loss: 1.5995 - val_accuracy: 0.2812\n",
      "Epoch 6/40\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 1.5799 - accuracy: 0.2879 - val_loss: 1.6049 - val_accuracy: 0.2188\n",
      "Epoch 7/40\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 1.6128 - accuracy: 0.2879 - val_loss: 1.4750 - val_accuracy: 0.2812\n",
      "Epoch 8/40\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.4660 - accuracy: 0.4091 - val_loss: 1.2605 - val_accuracy: 0.6250\n",
      "Epoch 9/40\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 1.2385 - accuracy: 0.4899 - val_loss: 1.1447 - val_accuracy: 0.6562\n",
      "Epoch 10/40\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 1.0973 - accuracy: 0.5580 - val_loss: 0.8789 - val_accuracy: 0.7188\n",
      "Epoch 11/40\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.9410 - accuracy: 0.6414 - val_loss: 0.8304 - val_accuracy: 0.7188\n",
      "Epoch 12/40\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.6474 - accuracy: 0.7879 - val_loss: 0.4033 - val_accuracy: 0.9688\n",
      "Epoch 13/40\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.4630 - accuracy: 0.8636 - val_loss: 0.2136 - val_accuracy: 0.9688\n",
      "Epoch 14/40\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.3957 - accuracy: 0.8990 - val_loss: 0.2060 - val_accuracy: 0.9375\n",
      "Epoch 15/40\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.3632 - accuracy: 0.8939 - val_loss: 0.1761 - val_accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2946 - accuracy: 0.9040 - val_loss: 0.1036 - val_accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3557 - accuracy: 0.9040 - val_loss: 0.2506 - val_accuracy: 0.9062\n",
      "Epoch 18/40\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2290 - accuracy: 0.9394 - val_loss: 0.3025 - val_accuracy: 0.9062\n",
      "Epoch 19/40\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.2604 - accuracy: 0.9091 - val_loss: 0.2013 - val_accuracy: 0.9375\n",
      "Epoch 20/40\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.2797 - accuracy: 0.8889 - val_loss: 0.1577 - val_accuracy: 0.9375\n",
      "Epoch 21/40\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2255 - accuracy: 0.9196 - val_loss: 0.2096 - val_accuracy: 0.9062\n",
      "Epoch 22/40\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1844 - accuracy: 0.9495 - val_loss: 0.1915 - val_accuracy: 0.9375\n",
      "Epoch 23/40\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2126 - accuracy: 0.9192 - val_loss: 0.1506 - val_accuracy: 0.9688\n",
      "Epoch 24/40\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1754 - accuracy: 0.9444 - val_loss: 0.2768 - val_accuracy: 0.9062\n",
      "Epoch 25/40\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.2259 - accuracy: 0.9107 - val_loss: 0.1483 - val_accuracy: 0.9688\n",
      "Epoch 26/40\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.2057 - accuracy: 0.9242 - val_loss: 0.1137 - val_accuracy: 0.9688\n",
      "Epoch 27/40\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1649 - accuracy: 0.9495 - val_loss: 0.0904 - val_accuracy: 0.9688\n",
      "Epoch 28/40\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1712 - accuracy: 0.9444 - val_loss: 0.1430 - val_accuracy: 0.9375\n",
      "Epoch 29/40\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.1816 - accuracy: 0.9444 - val_loss: 0.0782 - val_accuracy: 0.9688\n",
      "Epoch 30/40\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1546 - accuracy: 0.9394 - val_loss: 0.2004 - val_accuracy: 0.9062\n",
      "Epoch 31/40\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1124 - accuracy: 0.9747 - val_loss: 0.1380 - val_accuracy: 0.9688\n",
      "Epoch 32/40\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1460 - accuracy: 0.9343 - val_loss: 0.2077 - val_accuracy: 0.9062\n",
      "Epoch 33/40\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.1011 - accuracy: 0.9697 - val_loss: 0.0315 - val_accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0980 - accuracy: 0.9545 - val_loss: 0.0466 - val_accuracy: 0.9688\n",
      "Epoch 35/40\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0825 - accuracy: 0.9646 - val_loss: 0.0218 - val_accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0670 - accuracy: 0.9798 - val_loss: 0.0076 - val_accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.1351 - accuracy: 0.9646 - val_loss: 0.0909 - val_accuracy: 0.9688\n",
      "Epoch 38/40\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0895 - accuracy: 0.9596 - val_loss: 0.1442 - val_accuracy: 0.9375\n",
      "Epoch 39/40\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.1159 - accuracy: 0.9598 - val_loss: 0.0474 - val_accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.1001 - accuracy: 0.9596 - val_loss: 0.0387 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f5258181d00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set parameters\n",
    "img_width, img_height = 30, 30\n",
    "batch_size = 32\n",
    "\n",
    "# Define a simple CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(6, activation='softmax')  # Assuming 3 classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Data generators for training and validation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2, \n",
    "    horizontal_flip=True,  # Enable horizontal flip\n",
    "    rotation_range=90# Splitting data into 80% training and 20% validation\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/home/bule/projects/Dice/data/single_dices/single_dices/dice_classes',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    color_mode='grayscale',  # Specify grayscale images\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    '/home/bule/projects/Dice/data/single_dices/single_dices/dice_classes',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    color_mode='grayscale',  # Specify grayscale images\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    epochs=40  # Adjust the number of epochs according to your needs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 8ms/step\n",
      "[[9.9958140e-01 2.7688311e-05 3.9097472e-04 4.8002461e-12 6.2728878e-10\n",
      "  1.9545752e-11]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_prediction(model, file_path):\n",
    "    # Load the image file, converting it to a numpy array\n",
    "    img = image.load_img(file_path, target_size=(img_width, img_height), color_mode='grayscale')\n",
    "    img_array = image.img_to_array(img)\n",
    "\n",
    "    # Rescale the image (as we did for the training images)\n",
    "    img_array /= 255.0\n",
    "\n",
    "    # Add a new axis to make the image array compatible with the model (expects a batch)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # Make the prediction\n",
    "    prediction = model.predict(img_array)\n",
    "\n",
    "    return prediction\n",
    "\n",
    "# Usage\n",
    "# model should be the trained model you want to use for prediction\n",
    "# '/path/to/image.png' should be the path to the image you want to predict\n",
    "\n",
    "img_path = \"/home/bule/projects/Dice/data/single_dices/single_dices/dice_classes/1/dice25.png\"\n",
    "img = cv2.imread(img_path)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "prediction = make_prediction(model, img_path)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpq0qtty1e/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpq0qtty1e/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to TensorFlow Lite and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-04 19:38:39.640056: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-11-04 19:38:39.640069: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-11-04 19:38:39.640218: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpq0qtty1e\n",
      "2023-11-04 19:38:39.640958: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-11-04 19:38:39.640963: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpq0qtty1e\n",
      "2023-11-04 19:38:39.642671: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n",
      "2023-11-04 19:38:39.643204: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-11-04 19:38:39.670406: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpq0qtty1e\n",
      "2023-11-04 19:38:39.677270: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 37052 microseconds.\n",
      "2023-11-04 19:38:39.685690: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TensorFlow Lite model to a file\n",
    "with open('model_single_dices.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model converted to TensorFlow Lite and saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
